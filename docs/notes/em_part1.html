<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head><meta charset="utf-8">
  <meta name="generator" content="pandoc">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">
  <meta name="dcterms.date" content="2024-02-17">
  <title>EM Algorithm: Part 1</title>
  <style>
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    ul.task-list{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      width: 0.8em;
      margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
      vertical-align: middle;
    }
    /* CSS for citations */
    div.csl-bib-body { }
    div.csl-entry {
      clear: both;
      margin-bottom: 0em;
    }
    .hanging-indent div.csl-entry {
      margin-left:2em;
      text-indent:-2em;
    }
    div.csl-left-margin {
      min-width:2em;
      float:left;
    }
    div.csl-right-inline {
      margin-left:2em;
      padding-left:1em;
    }
    div.csl-indent {
      margin-left: 2em;
    }  </style>
  <link rel="stylesheet" href="assets/css/sakura.css">
  <link rel="stylesheet" href="../assets/css/sakura.css">
  <link rel="stylesheet" href="../assets/css/site.css">
  <script src="../assets/js/toc-nav.js"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>
  <!--[if lt IE 9]>
    <script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.3/html5shiv-printshiv.min.js"></script>
    <![endif]-->
</head>
<body>
        <header id="title-block-header">
    <h1 class="title">EM Algorithm: Part 1</h1>
                  <p class="date">2024-02-17</p>
        </header>
      <div class="container">
            <main class="article">
        <p><em>This note is the first in a three part series on the expectation maximization algorithm.</em></p>
        <p>The expectation-maximization (EM) algorithm is a two-step iterative process for estimating the parameters in a latent variable model.</p>
        <p>Consider an observable random variable, <span class="math inline">\(X\)</span>, with latent classification <span class="math inline">\(Z\)</span>. We seek to estimate a vector of parameters, <span class="math inline">\(\theta\)</span>, by maximizing the marginal log-likelihood formed by marginalizing over the support of <span class="math inline">\(Z\)</span>.</p>
        <p><span class="math display">\[\begin{equation}
        \ell(\theta | X) = \log \left(\int p(X, Z | \theta) \; d\mu(z)\right)
        \end{equation}\]</span></p>
        <p>Even though we can decompose the joint probability function to <span class="math inline">\(p(X, Z | \theta) = p(Z | \theta) p(X | Z, \theta)\)</span>, as we’ll see in part 2 when discussing mixture models, this nonetheless tends to lead to an intractable maximization problem since we’re marginalizing over the latent variable within the natural logarithm.</p>
        <p>Instead, we turn to iteratively maximizing the conditional expectation of the joint log-likelihood with respect to the latent variable <span class="math inline">\(Z\)</span>. Put simply, we don’t directly observe <span class="math inline">\(Z\)</span>, so instead we form a best guess by taking the conditional expectation given our data and our current values of <span class="math inline">\(\theta\)</span>, <em>i.e.</em> the E-step, and then update <span class="math inline">\(\theta\)</span> by maximizing the resulting equation, the M-step. Rinse and repeat.</p>
        <p>More formally, let the objective function be the following:</p>
        <p><span class="math display">\[\begin{equation}
        Q(\theta^{(t)}, \theta) = \mathbb{E}_{Z} \left[ \log p(X, Z | \theta) | X, \theta^{(t)} \right]
        \end{equation}\]</span></p>
        <p>Expanding out the objective function makes clear the different components.</p>
        <span class="math display">\[\begin{aligned}
        Q(\theta^{(t)}, \theta) &amp; = \int  p(z | X, \theta^{(t)}) \log p(X, Z, | \theta) \; d\mu(z) \\
        &amp; = \int p(z | X, \theta^{(t)}) \left( \log p(Z | \theta) + \log p(X | Z, \theta) \right) \; d\mu(z)
        \end{aligned}\]</span>
        <p>We first select some initial values for <span class="math inline">\(\theta\)</span>. Then, in the E-step we calculate <span class="math inline">\(p(z | X , \theta^{(t)}) \; \forall z \in S\)</span> where <span class="math inline">\(S\)</span> is the support of <span class="math inline">\(Z\)</span>. In the M-step, we update <span class="math inline">\(\theta\)</span> by maximizing the objective function.</p>
        <p><span class="math display">\[\begin{equation}
        \theta^{(t+1)} = \text{arg max}_{\theta \in \Theta} Q(\theta^{(t)}, \theta)
        \end{equation}\]</span></p>
        <p>We alternate between the E-step and M-step until some convergence statistic is satisfied, for example if <span class="math inline">\(| \ell(\theta^{(t+1)} | X) - \ell(\theta^{(t)} | X) | &lt; \epsilon\)</span> for some <span class="math inline">\(\epsilon &gt; 0\)</span>.</p>
        <p>Note, the EM algorithm does not guarantee that we will find the global maximum for <span class="math inline">\(\theta\)</span>; however, we are guaranteed to monotonically increase the marginal log-likelihood for <span class="math inline">\(X\)</span>.</p>
        <p>By Jensen’s Inequality, for a concave function <span class="math inline">\(f(\cdot)\)</span>:</p>
        <p><span class="math display">\[\begin{equation}
        \mathbb{E} \left[ f(X) \right ] \leq f \left( \mathbb{E}X \right)
        \end{equation}\]</span></p>
        <p>Within the context of the EM algorithm multiply the marginal likelihood by <span class="math inline">\(\frac{p(Z | X, \theta)}{p(Z | X, \theta)}\)</span>.</p>
        <span class="math display">\[\begin{aligned}
        \log p(X | \theta) &amp; = \log \int p(X, Z | \theta) \cfrac{p(Z | X, \theta^{(t)})}{p(Z | X, \theta^{(t)})} \; d\mu(z) \\
        &amp; = \log \mathbb{E}_Z \left[ \cfrac{p(X, Z | \theta)}{p(Z | X, \theta^{(t)})} | X, \theta^{(t)} \right] \\
        &amp; \geq \mathbb{E}_Z \left[ \log \cfrac{p(X, Z | \theta)}{p(Z | X, \theta^{(t)})} | X, \theta^{(t)} \right] \\
        &amp; = \mathbb{E}_Z \left[ \log p(X, Z | \theta) | X, \theta^{(t)} \right] - \mathbb{E}_Z \left[ \log p(Z | X, \theta^{(t)}) | X, \theta^{(t)} \right] \\
        &amp; = Q(\theta^{(t)}, \theta) - \mathbb{E}_Z \left[ \log p(Z | X, \theta^{(t)}) | X, \theta^{(t)} \right]
        \end{aligned}\]</span>
        <p>This provides the lower bound for the marginal log likelihood of <span class="math inline">\(X\)</span>. Since <span class="math inline">\(\theta^{(t+1)} \geq \theta^{(t)}\)</span> when maximizing <span class="math inline">\(Q(\theta^{(t)}, \theta)\)</span>, then <span class="math inline">\(\log p(X | \theta)\)</span> will monotonically increase for each M-step as the second term will effectively be a constant after the E-step.</p>
        <section id="references" class="level1 unnumbered">
        <h1 class="unnumbered">References</h1>
        <div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0" role="list">
        <div id="ref-bishop2006pattern" class="csl-entry" role="listitem">
        Bishop, Christopher. 2006. <em>Pattern Recognition and Machine Learning</em>. Springer.
        </div>
        <div id="ref-hastie2009elements" class="csl-entry" role="listitem">
        Hastie, Trevor, Robert Tibshirani, Jerome H Friedman, and Jerome H Friedman. 2009. <em>The Elements of Statistical Learning: Data Mining, Inference, and Prediction</em>. Vol. 2. Springer.
        </div>
        </div>
        </section>
      </main>
    </div>


</body></html>
